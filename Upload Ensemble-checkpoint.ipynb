{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports of package used in the \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv(\"X_train.csv\",sep=';',index_col=\"Id\") #_sqprice for price per square feet\n",
    "y_train=pd.read_csv(\"y_train.csv\",sep=';',index_col=\"Id\")\n",
    "#X_test=pd.read_csv(\"X_test.csv\",sep=';',index_col=\"Id\")\n",
    "X_train_split1, X_test, y_train_split1, y_test = train_test_split(X_train,y_train, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to evaluate the fit (rmse) of the prediciton\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def evaluate_fit(y_test,predictions):\n",
    "    rmse=np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports: decisiontreeregressor, np,pandas, train_test_split\n",
    "def ensemble_fct(random_state=42,n=20,test_size=0.2,max_features=None,max_depth=6,report_mse=False):\n",
    "    \"\"\"This function draws subsamples of the training set, trains a tree to every subsample and generates an\n",
    "    ensembles of these trees\n",
    "    \n",
    "    +++++parameters+++++\n",
    "    random_state: [None or any integer]\n",
    "    n: [int] specifies how many subsamples/trees should be used\n",
    "    test_size: [float (0,1)] - specifies the proportion of the training data should be used as subsample\n",
    "    max_features:[int or float(0,1)] specifies how many features should be considered at splits. this is used in random forest methods \n",
    "    to decorrelate trees\n",
    "    max_depth: [int] - specifies the maximum depth of the single trees\n",
    "    report_mse: reports the mse of every single predictors and every ensemble\n",
    "    \"\"\"\n",
    "    \n",
    "    # drawing randomly 'n' random states from (0,2000)\n",
    "    states=[]\n",
    "    while len(states)<n:\n",
    "        candidate=np.random.randint(0,2000)\n",
    "        if candidate not in states:\n",
    "            states.append(candidate)\n",
    "    print(\"generated \"+str(n)+\" random states\")\n",
    "    \n",
    "    #initialize the tree with the prespecified parameters and define a dataframe to store the forecasts of every tree\n",
    "    dtree=DecisionTreeRegressor(random_state=random_state,max_depth=max_depth,max_features=max_features) #max_features=80 \n",
    "    all_predictions=pd.DataFrame(index=X_test.index)\n",
    "    \n",
    "    #drawing n subsamples each time from tull training dataset, fitting a tree to the subsample\n",
    "    #    and use the tree to make prediction for every tree\n",
    "    for i in range(n):\n",
    "        X_train_split, X_eval, y_train_split, y_eval = train_test_split(X_train_split1,y_train_split1, test_size=test_size,random_state=states[i])\n",
    "        dtree.fit(X_train_split, y_train_split)\n",
    "        all_predictions[str(i)]=dtree.predict(X_test)\n",
    "    \n",
    "    #evaluate the fit of single tree predictions, returning contains all output variables for further use\n",
    "    returning={}\n",
    "    mse_tree=[]\n",
    "    for i in range(len(states)):\n",
    "        mse_t=evaluate_fit(y_test,all_predictions[str(i)])\n",
    "        mse_tree.append(mse_t)\n",
    "        if report_mse==True:\n",
    "            print(str(i)+\". Tree: \"+str(mse_t))\n",
    "    returning[\"mse_tree\"]=mse_tree\n",
    "    print(\"generated \"+str(n)+\" predictions\")\n",
    "    \n",
    "    # combine the individual predictions to n ensembles (first ensemble is just single tree)\n",
    "    ensemble=pd.DataFrame(index=X_test.index)\n",
    "    labels=list(all_predictions.columns.values)\n",
    "    ensemble[\"Ensemble0\"]=(all_predictions[\"0\"])\n",
    "    for i in range(1,len(labels)):\n",
    "        ensemble[\"Ensemble\"+labels[i]]=ensemble[\"Ensemble\"+str(i-1)]+all_predictions[labels[i]]\n",
    "    \n",
    "    #divide the ensembles by the number of trees used for the ensemble predictions\n",
    "    ensemble_labels=list(ensemble.columns.values)\n",
    "    counter=1\n",
    "    for i in ensemble_labels:\n",
    "        ensemble[i]=ensemble[i]/counter\n",
    "        counter+=1\n",
    "    print(\"generated \"+str(n)+\" ensembles\")\n",
    "    \n",
    "    #evaluate the fit of ensembles\n",
    "    mse_ensemble=[]\n",
    "    for i in ensemble.columns.values:\n",
    "        mse_e=evaluate_fit(y_test,ensemble[str(i)])\n",
    "        if report_mse==True:\n",
    "            print(i+\": \"+str(mse_e))\n",
    "        mse_ensemble.append(mse_e)\n",
    "        \n",
    "    #return a dictionary with all the relevant outputs for further use    \n",
    "    returning[\"mse_ensemble\"]=mse_ensemble\n",
    "    returning[\"trees\"]=all_predictions\n",
    "    returning[\"ensemble\"]=ensemble\n",
    "    returning[\"states\"]=states\n",
    "    \n",
    "    \n",
    "    return returning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
